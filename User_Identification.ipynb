{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1YUMXK2ZmRIZI0K3RG9utcgZqshGzI_Ii",
      "authorship_tag": "ABX9TyNY37h4nHw8DLwT+ykMC7Mx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arham5siddiqui/Mitigating-Linkability-Attacks-through-Differential-Privacy-enabled-Neural-Network-Training/blob/main/User_Identification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "3VNPgEjY8N-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shuffling a Train file in chunks, in order include multiple Classes(multiple user data) in every chunk."
      ],
      "metadata": {
        "id": "6BOGaZGq5I37"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "input_file_path = \"/content/drive/MyDrive/MSc Project/Formated_Data/Experiment_1/TrainTestDataset/Reduced_Combined_Train.csv\"\n",
        "output_file_path = \"/content/drive/MyDrive/MSc Project/Formated_Data/Experiment_1/TrainTestDataset/Shuffled_Reduced_Combined_Train.csv\"\n",
        "\n",
        "chunk_size = 10000  # Choose a reasonable chunk size\n",
        "\n",
        "# Initialize writing to CSV\n",
        "first_one = True\n",
        "\n",
        "# Read and shuffle each chunk, then append to new CSV\n",
        "for chunk in pd.read_csv(input_file_path, chunksize=chunk_size):\n",
        "    shuffled_chunk = chunk.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "    if first_one:\n",
        "        shuffled_chunk.to_csv(output_file_path, mode='w', index=False)\n",
        "        first_one = False\n",
        "    else:\n",
        "        shuffled_chunk.to_csv(output_file_path, mode='a', header=False, index=False)\n"
      ],
      "metadata": {
        "id": "MhMIgyeE5Jjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using: all columns except labels 'userID' and 'videoID', as features.\n",
        "First each classifier is trained,\n",
        "and then tested, with the output printing accuracy table for each classifier."
      ],
      "metadata": {
        "id": "8wybz58msdt_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Initialize classifiers\n",
        "classifiers = {\n",
        "    \"SGD\": SGDClassifier(),\n",
        "    \"GaussianNB\": GaussianNB(),\n",
        "    \"RandomForest\": RandomForestClassifier(),\n",
        "    \"DecisionTree\": DecisionTreeClassifier(),\n",
        "    \"KNN\": KNeighborsClassifier()\n",
        "}\n",
        "\n",
        "# Initialize a flag to indicate if each classifier has been fitted\n",
        "fitted = {name: False for name in classifiers.keys()}\n",
        "\n",
        "# Initialize empty dictionary to store accuracy\n",
        "accuracy_dict = {'Classifier': [], 'Dataset': [], 'Accuracy': []}\n",
        "\n",
        "# Initialize empty list to store train accuracy\n",
        "train_accuracy_list = []\n",
        "\n",
        "# Initialize final dictionary to store average accuracies\n",
        "final_dict = {'Classifier': [], 'Dataset': [], 'Accuracy': [], 'Train Accuracy': []}\n",
        "\n",
        "# Initialize the list of dataset types\n",
        "dataset_types = ['Quaternion', 'Euler', 'Yaw']  # Adding actual dataset types\n",
        "\n",
        "# Define chunk size\n",
        "chunk_size = 10 ** 6  # Adjust based on your available memory\n",
        "subset_size = 10 ** 4  # Size for training classifiers that don't support partial_fit\n",
        "\n",
        "# Read a small portion of the file to get the unique classes\n",
        "unique_classes = set()\n",
        "for chunk in pd.read_csv(\"/content/drive/MyDrive/MSc Project/Formated_Data/Experiment_1/TrainTestDataset/Shuffled_Reduced_Combined_Train.csv\", chunksize=chunk_size):\n",
        "    unique_classes.update(chunk['userID'])\n",
        "unique_classes = np.array(list(unique_classes))\n",
        "\n",
        "\n",
        "# Function to train classifiers that support partial_fit\n",
        "def train_partial_fit(chunk):\n",
        "    X_train = chunk.drop(['userID', 'videoID'], axis=1)\n",
        "    y_train = chunk['userID']\n",
        "    if len(np.unique(y_train)) > 1:  # Check for multiple unique classes\n",
        "        for name, clf in classifiers.items():\n",
        "            if hasattr(clf, 'partial_fit'):\n",
        "                clf.partial_fit(X_train, y_train, classes=unique_classes)\n",
        "                fitted[name] = True  # Mark as fitted\n",
        "\n",
        "# Function to test classifiers\n",
        "def test_classifiers(chunk, dataset_type):\n",
        "    X_test = chunk.drop(['userID', 'videoID'], axis=1)\n",
        "    y_test = chunk['userID']\n",
        "    for name, clf in classifiers.items():\n",
        "        if fitted[name]:  # Only test if the classifier was fitted\n",
        "            y_pred = clf.predict(X_test)\n",
        "            accuracy = accuracy_score(y_test, y_pred)\n",
        "            accuracy_dict['Classifier'].append(name)\n",
        "            accuracy_dict['Dataset'].append(dataset_type)\n",
        "            accuracy_dict['Accuracy'].append(accuracy)\n",
        "\n",
        "\n",
        "# Training loop for classifiers that support partial_fit\n",
        "for chunk in pd.read_csv(\"/content/drive/MyDrive/MSc Project/Formated_Data/Experiment_1/TrainTestDataset/Shuffled_Reduced_Combined_Train.csv\", chunksize=chunk_size):\n",
        "    train_partial_fit(chunk)\n",
        "\n",
        "# Fit classifiers that do not support partial_fit on a smaller subset\n",
        "subset = pd.read_csv(\"/content/drive/MyDrive/MSc Project/Formated_Data/Experiment_1/TrainTestDataset/Shuffled_Reduced_Combined_Train.csv\", nrows=subset_size)\n",
        "X_subset = subset.drop(['userID', 'videoID'], axis=1)\n",
        "y_subset = subset['userID']\n",
        "for name, clf in classifiers.items():\n",
        "    if not fitted[name]:\n",
        "        clf.fit(X_subset, y_subset)\n",
        "        fitted[name] = True\n",
        "        train_accuracy = clf.score(X_subset, y_subset)\n",
        "        train_accuracy_list.append((name, train_accuracy))\n",
        "\n",
        "# Test the classifiers\n",
        "for chunk in pd.read_csv(\"/content/drive/MyDrive/MSc Project/Formated_Data/Experiment_1/TrainTestDataset/Reduced_Combined_Test.csv\", chunksize=chunk_size):\n",
        "    test_classifiers(chunk, 'Test')\n",
        "\n",
        "# Average the test accuracy and fill in the dictionary for DataFrame creation\n",
        "for name in classifiers.keys():\n",
        "    indices = [i for i, x in enumerate(accuracy_dict['Classifier']) if x == name]\n",
        "    if indices:\n",
        "        avg_accuracy = np.mean([accuracy_dict['Accuracy'][i] for i in indices])\n",
        "        final_dict['Classifier'].append(name)\n",
        "        final_dict['Dataset'].append('Test')  # This will be 'Test' for all classifiers in this case\n",
        "        final_dict['Accuracy'].append(avg_accuracy)\n",
        "        final_dict['Train Accuracy'].append(np.nan)  # Placeholder for train accuracy\n",
        "\n",
        "# Incorporate the train_accuracy_list into the final_dict\n",
        "for name, train_accuracy in train_accuracy_list:\n",
        "    if name in final_dict['Classifier']:\n",
        "        idx = final_dict['Classifier'].index(name)\n",
        "        final_dict['Train Accuracy'][idx] = train_accuracy\n",
        "\n",
        "# Save results to CSV\n",
        "results_df = pd.DataFrame(final_dict)\n",
        "results_df.to_csv(\"/content/drive/MyDrive/MSc Project/Formated_Data/Experiment_1/TrainTestDataset/results.csv\", index=False)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UuGfD-Kh_ksG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Initialize classifiers\n",
        "classifiers = {\n",
        "    \"SGD\": SGDClassifier(),\n",
        "    \"GaussianNB\": GaussianNB(),\n",
        "    \"RandomForest\": RandomForestClassifier(),\n",
        "    \"DecisionTree\": DecisionTreeClassifier(),\n",
        "    \"KNN\": KNeighborsClassifier()\n",
        "}\n",
        "\n",
        "# Initialize a flag to indicate if each classifier has been fitted\n",
        "fitted = {name: False for name in classifiers.keys()}\n",
        "\n",
        "# Initialize empty dictionary to store accuracy\n",
        "accuracy_dict = {'Classifier': [], 'Dataset': [], 'Accuracy': []}\n",
        "\n",
        "# Initialize empty list to store train accuracy\n",
        "train_accuracy_list = []\n",
        "\n",
        "# Initialize final dictionary to store average accuracies\n",
        "final_dict = {'Classifier': [], 'Dataset': [], 'Accuracy': [], 'Train Accuracy': []}\n",
        "\n",
        "# Initialize the list of dataset types\n",
        "dataset_types = ['Quaternion', 'Euler', 'Yaw']  # Adding actual dataset types\n",
        "\n",
        "# Initialize running sum and count for training accuracy for classifiers that support partial_fit\n",
        "train_accuracy_sum = {name: 0.0 for name in classifiers.keys()}\n",
        "train_accuracy_count = {name: 0 for name in classifiers.keys()}\n",
        "\n",
        "# Define chunk size\n",
        "chunk_size = 10 ** 6  # Adjust based on your available memory\n",
        "subset_size = 10 ** 4  # Size for training classifiers that don't support partial_fit\n",
        "\n",
        "# Read a small portion of the file to get the unique classes\n",
        "unique_classes = set()\n",
        "for chunk in pd.read_csv(\"/content/drive/MyDrive/MSc Project/Formated_Data/Experiment_1/TrainTestDataset/Shuffled_Reduced_Combined_Train.csv\", chunksize=chunk_size):\n",
        "    unique_classes.update(chunk['userID'])\n",
        "unique_classes = np.array(list(unique_classes))\n",
        "\n",
        "\n",
        "def train_partial_fit(chunk):\n",
        "    X_train = chunk.drop(['userID', 'videoID'], axis=1)\n",
        "    y_train = chunk['userID']\n",
        "    if len(np.unique(y_train)) > 1:\n",
        "        for name, clf in classifiers.items():\n",
        "            if hasattr(clf, 'partial_fit'):\n",
        "                clf.partial_fit(X_train, y_train, classes=unique_classes)\n",
        "                train_accuracy = clf.score(X_train, y_train)  # Calculate train accuracy for this chunk\n",
        "                train_accuracy_sum[name] += train_accuracy * len(y_train)  # Update the running sum\n",
        "                train_accuracy_count[name] += len(y_train)  # Update the count\n",
        "                fitted[name] = True\n",
        "\n",
        "# Function to test classifiers\n",
        "def test_classifiers(chunk, dataset_type):\n",
        "    X_test = chunk.drop(['userID', 'videoID'], axis=1)\n",
        "    y_test = chunk['userID']\n",
        "    for name, clf in classifiers.items():\n",
        "        if fitted[name]:  # Only test if the classifier was fitted\n",
        "            y_pred = clf.predict(X_test)\n",
        "            accuracy = accuracy_score(y_test, y_pred)\n",
        "            accuracy_dict['Classifier'].append(name)\n",
        "            accuracy_dict['Dataset'].append(dataset_type)\n",
        "            accuracy_dict['Accuracy'].append(accuracy)\n",
        "\n",
        "\n",
        "# Training loop for classifiers that support partial_fit\n",
        "for chunk in pd.read_csv(\"/content/drive/MyDrive/MSc Project/Formated_Data/Experiment_1/TrainTestDataset/Shuffled_Reduced_Combined_Train.csv\", chunksize=chunk_size):\n",
        "    train_partial_fit(chunk)\n",
        "\n",
        "# Fit classifiers that do not support partial_fit on a smaller subset\n",
        "subset = pd.read_csv(\"/content/drive/MyDrive/MSc Project/Formated_Data/Experiment_1/TrainTestDataset/Shuffled_Reduced_Combined_Train.csv\", nrows=subset_size)\n",
        "X_subset = subset.drop(['userID', 'videoID'], axis=1)\n",
        "y_subset = subset['userID']\n",
        "for name, clf in classifiers.items():\n",
        "    if not fitted[name]:\n",
        "        clf.fit(X_subset, y_subset)\n",
        "        fitted[name] = True\n",
        "        train_accuracy = clf.score(X_subset, y_subset)\n",
        "        train_accuracy_list.append((name, train_accuracy))\n",
        "\n",
        "# Test the classifiers\n",
        "for chunk in pd.read_csv(\"/content/drive/MyDrive/MSc Project/Formated_Data/Experiment_1/TrainTestDataset/Reduced_Combined_Test.csv\", chunksize=chunk_size):\n",
        "    test_classifiers(chunk, 'Test')\n",
        "\n",
        "\n",
        "# Average the training accuracy for classifiers that support partial_fit\n",
        "for name in classifiers.keys():\n",
        "    if train_accuracy_count[name] > 0:\n",
        "        avg_train_accuracy = train_accuracy_sum[name] / train_accuracy_count[name]\n",
        "        final_dict['Train Accuracy'].append(avg_train_accuracy)\n",
        "    else:\n",
        "        final_dict['Train Accuracy'].append(np.nan)  # Placeholder for classifiers that were not fitted\n",
        "\n",
        "print({key: len(value) for key, value in final_dict.items()})\n",
        "\n",
        "# Save results to CSV\n",
        "results_df = pd.DataFrame(final_dict)\n",
        "results_df.to_csv(\"/content/drive/MyDrive/MSc Project/Formated_Data/Experiment_1/TrainTestDataset/results.csv\", index=False)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 616
        },
        "id": "XuH1eDbyFOBK",
        "outputId": "6805bbdc-d41c-4520-9298-06815fa391db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/naive_bayes.py:514: RuntimeWarning: divide by zero encountered in log\n",
            "  jointi = np.log(self.class_prior_[i])\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/naive_bayes.py:514: RuntimeWarning: divide by zero encountered in log\n",
            "  jointi = np.log(self.class_prior_[i])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Classifier': 0, 'Dataset': 0, 'Accuracy': 0, 'Train Accuracy': 5}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-5b5942e9e359>\u001b[0m in \u001b[0;36m<cell line: 105>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;31m# Save results to CSV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m \u001b[0mresults_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0mresults_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/MSc Project/Formated_Data/Experiment_1/TrainTestDataset/results.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    662\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m             \u001b[0;31m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 664\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmanager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    665\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsolidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;31m# figure out the index, if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_extract_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    664\u001b[0m             \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 666\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"All arrays must be of the same length\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    667\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhave_dicts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: All arrays must be of the same length"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "import os\n",
        "\n",
        "def shuffle_large_file(file_path, temp_path, chunk_size=10000):\n",
        "    # Step 1: Sort the original file based on 'userID' and 'videoID'\n",
        "    sorted_chunks = []\n",
        "    for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
        "        sorted_chunk = chunk.sort_values(by=['userID', 'videoID'])\n",
        "        sorted_chunks.append(sorted_chunk)\n",
        "\n",
        "    sorted_df = pd.concat(sorted_chunks, ignore_index=True)\n",
        "    sorted_df.to_csv(temp_path, index=False)\n",
        "\n",
        "    # Step 2: Shuffle within each chunk and save to temporary file\n",
        "    shuffled_chunks = []\n",
        "    for chunk in pd.read_csv(temp_path, chunksize=chunk_size):\n",
        "        grouped = chunk.groupby(['userID', 'videoID'])\n",
        "        groups = [group for name, group in grouped]\n",
        "        random.shuffle(groups)\n",
        "        shuffled_chunk = pd.concat(groups).reset_index(drop=True)\n",
        "        shuffled_chunks.append(shuffled_chunk)\n",
        "\n",
        "    # Step 3: Shuffle the chunks and write to final file\n",
        "    random.shuffle(shuffled_chunks)\n",
        "    final_df = pd.concat(shuffled_chunks, ignore_index=True)\n",
        "    final_df.to_csv(file_path, index=False)\n",
        "\n",
        "    # Remove the temporary file\n",
        "    os.remove(temp_path)\n",
        "\n",
        "# List of file paths to be shuffled\n",
        "file_paths = [\n",
        "    '/content/drive/MyDrive/MSc Project/Formated_Data/Experiment_1/TrainTestDataset/Reduced_Combined_Train.csv',\n",
        "    # Add more file paths if needed\n",
        "]\n",
        "\n",
        "# Temporary file path\n",
        "temp_path = '/content/drive/MyDrive/MSc Project/Formated_Data/Experiment_1/TrainTestDataset/temp_sorted.csv'\n",
        "\n",
        "# Shuffle each file in chunks\n",
        "for file_path in file_paths:\n",
        "    shuffle_large_file(file_path, temp_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nNakzL47hsK3",
        "outputId": "1e05e5d4-00fb-47b1-87b3-7358c4c219a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1777\u001b[0m                     \u001b[0mcol_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1778\u001b[0;31m                 \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1779\u001b[0m                     \u001b[0mnrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [Errno 107] Transport endpoint is not connected",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-6cae2c542fbf>\u001b[0m in \u001b[0;36m<cell line: 42>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m# Shuffle each file in chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile_paths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mshuffle_large_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-1-6cae2c542fbf>\u001b[0m in \u001b[0;36mshuffle_large_file\u001b[0;34m(file_path, temp_path, chunk_size)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Step 1: Sort the original file based on 'userID' and 'videoID'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0msorted_chunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0msorted_chunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'userID'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'videoID'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0msorted_chunks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted_chunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1696\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1697\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1698\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1699\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1700\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mget_chunk\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m   1808\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m             \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnrows\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_currow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1810\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1812\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1780\u001b[0m                 )\n\u001b[1;32m   1781\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1782\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1783\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1784\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mclose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1446\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1447\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mclose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreated_handles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreated_handles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m             \u001b[0mhandle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreated_handles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_wrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [Errno 107] Transport endpoint is not connected"
          ]
        }
      ]
    }
  ]
}