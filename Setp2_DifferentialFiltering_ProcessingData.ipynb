{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Os4BrvwE6udmO8cmGrAyI7Lne1oJ86_t",
      "authorship_tag": "ABX9TyPWz89Q45PRW7NQ5RUz7Rm+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arham5siddiqui/Mitigating-Linkability-Attacks-through-Differential-Privacy-enabled-Neural-Network-Training/blob/main/Setp2_DifferentialFiltering_ProcessingData.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import shutil\n",
        "from scipy.spatial.transform import Rotation as R"
      ],
      "metadata": {
        "id": "zRqip6_BBWig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Differential Filtering Function"
      ],
      "metadata": {
        "id": "ZBIP-cXZ7F16"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Function to compute Hamilton product\n",
        "def hamilton_product(q1, q2):\n",
        "    w1, x1, y1, z1 = q1\n",
        "    w2, x2, y2, z2 = q2\n",
        "    w = w1 * w2 - x1 * x2 - y1 * y2 - z1 * z2\n",
        "    x = w1 * x2 + x1 * w2 + y1 * z2 - z1 * y2\n",
        "    y = w1 * y2 + y1 * w2 + z1 * x2 - x1 * z2\n",
        "    z = w1 * z2 + z1 * w2 + x1 * y2 - y1 * x2\n",
        "    return np.array([w, x, y, z])\n",
        "\n",
        "# Process each CSV file\n",
        "def process_file(file_path, output_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Compute Quaternion Angular Velocity and Angular Acceleration\n",
        "    quaternions = df[['UnitQuaternion.w', 'UnitQuaternion.x', 'UnitQuaternion.y', 'UnitQuaternion.z']].values\n",
        "    quat_velocity = np.zeros_like(quaternions)\n",
        "    quat_accel = np.zeros_like(quaternions)\n",
        "\n",
        "    for t in range(1, len(quaternions)):\n",
        "        quat_velocity[t] = hamilton_product(quaternions[t], np.conjugate(quaternions[t-1]))\n",
        "        if t > 1:\n",
        "            quat_accel[t] = hamilton_product(quat_velocity[t], np.conjugate(quat_velocity[t-1]))\n",
        "\n",
        "    df['QuatAngVelocity.w'], df['QuatAngVelocity.x'], df['QuatAngVelocity.y'], df['QuatAngVelocity.z'] = quat_velocity.T\n",
        "    df['QuatAngAccel.w'], df['QuatAngAccel.x'], df['QuatAngAccel.y'], df['QuatAngAccel.z'] = quat_accel.T\n",
        "\n",
        "    # Compute Euler Angular Velocity and Angular Acceleration (assuming columns are 'yaw', 'pitch', 'roll')\n",
        "    eulers = df[['yaw', 'pitch', 'roll']].values\n",
        "    euler_velocity = np.diff(eulers, axis=0, prepend=np.zeros((1, eulers.shape[1])))\n",
        "    euler_accel = np.diff(euler_velocity, axis=0, prepend=np.zeros((1, euler_velocity.shape[1])))\n",
        "\n",
        "    df['EulerAngVelocity.yaw'], df['EulerAngVelocity.pitch'], df['EulerAngVelocity.roll'] = euler_velocity.T\n",
        "    df['EulerAngAccel.yaw'], df['EulerAngAccel.pitch'], df['EulerAngAccel.roll'] = euler_accel.T\n",
        "\n",
        "    # Save the updated DataFrame\n",
        "    df.to_csv(output_path, index=False)\n",
        "\n",
        "# Main code to loop through all users and files\n",
        "base_input_folder = '/content/drive/MyDrive/MSc Project/Formated_Data/Experiment_1/TrainTestDataset/'  # Update with your folder path\n",
        "base_output_folder = '//content/drive/MyDrive/MSc Project/Formated_Data/Experiment_1/TrainTestDataset/'  # Update with your output folder path\n",
        "\n",
        "for folder_type in ['Train', 'Test']:\n",
        "    for user_id in range(1, 49):  # Looping through 48 users\n",
        "        for video_id in range(9):  # Looping through 9 videos\n",
        "            input_file = f\"{base_input_folder}/{folder_type}/{user_id}_{folder_type.lower()}_resampled_video_{video_id}.csv\"\n",
        "            output_file = f\"{base_output_folder}/{folder_type}/{user_id}_{folder_type.lower()}_resampled_video_{video_id}.csv\"\n",
        "\n",
        "            if os.path.exists(input_file):\n",
        "                process_file(input_file, output_file)\n",
        "            else:\n",
        "                print(f\"File {input_file} not found.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "nftJFnRx7DaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Extraction (t-second splits as 1) - Processing Data"
      ],
      "metadata": {
        "id": "E4gfWeAU7LSz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def extract_features(df):\n",
        "    df['PlaybackTime'] = pd.to_timedelta(df['PlaybackTime'], unit='s')  # Convert to timedelta\n",
        "    df = df.dropna(subset=['PlaybackTime'])  # Drop rows where PlaybackTime is NaT\n",
        "    df.sort_values(by='PlaybackTime', inplace=True)  # Sort by PlaybackTime\n",
        "    df.set_index('PlaybackTime', inplace=True)\n",
        "\n",
        "    feature_columns = [col for col in df.columns if col not in ['Timestamp', 'PlaybackTime', 'userID', 'videoID']]\n",
        "\n",
        "    for col in feature_columns:\n",
        "        df[f'{col}_max'] = df[col].rolling(window='1s').max()\n",
        "        df[f'{col}_min'] = df[col].rolling(window='1s').min()\n",
        "        df[f'{col}_median'] = df[col].rolling(window='1s').median()\n",
        "        df[f'{col}_mean'] = df[col].rolling(window='1s').mean()\n",
        "        df[f'{col}_std'] = df[col].rolling(window='1s').std()\n",
        "\n",
        "    df.reset_index(inplace=True)\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def process_directory(input_folder, output_folder):\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    for filename in os.listdir(input_folder):\n",
        "        if filename.endswith('.csv'):\n",
        "            df = pd.read_csv(os.path.join(input_folder, filename))\n",
        "            df = extract_features(df)\n",
        "            new_filename = f\"processed_{filename}\"\n",
        "            df.to_csv(os.path.join(output_folder, new_filename), index=False)\n",
        "\n",
        "# Process Train and Test folders\n",
        "process_directory('/content/drive/MyDrive/MSc Project/Formated_Data/Experiment_1/TrainTestDataset/Train', '/content/drive/MyDrive/MSc Project/Formated_Data/Experiment_1/TrainTestDataset/Processed_Train')\n",
        "process_directory('/content/drive/MyDrive/MSc Project/Formated_Data/Experiment_1/TrainTestDataset/Test', '/content/drive/MyDrive/MSc Project/Formated_Data/Experiment_1/TrainTestDataset/Processed_Test')\n"
      ],
      "metadata": {
        "id": "ZucC9I0g1gPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Additional Features for Video and User Classification"
      ],
      "metadata": {
        "id": "LyqqMECu--w7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Function to calculate angle between two vectors\n",
        "def angle_between(v1, v2):\n",
        "    v1_u = v1 / (np.linalg.norm(v1) + 1e-8)  # Adding a small constant to avoid division by zero\n",
        "    v2_u = v2 / (np.linalg.norm(v2) + 1e-8)\n",
        "    return np.arccos(np.clip(np.dot(v1_u, v2_u), -1.0, 1.0))\n",
        "\n",
        "# Function for advanced feature extraction\n",
        "def advanced_feature_extraction(df):\n",
        "    quaternion_cols = [col for col in df.columns if 'Quaternion' in col]\n",
        "    euler_cols = [col for col in df.columns if col.lower() in ['yaw', 'pitch', 'roll']]\n",
        "\n",
        "    # Calculate angles between Quaternion vectors\n",
        "    for i in range(len(quaternion_cols)):\n",
        "        for j in range(i+1, len(quaternion_cols)):\n",
        "            col1, col2 = quaternion_cols[i], quaternion_cols[j]\n",
        "            col1_pos = df.columns.get_loc(col1)\n",
        "            col2_pos = df.columns.get_loc(col2)\n",
        "            df[f'QuaternionAngle_{col1}_{col2}'] = df.apply(lambda row: angle_between(row.iloc[col1_pos:col1_pos+4].values, row.iloc[col2_pos:col2_pos+4].values), axis=1)\n",
        "\n",
        "    # Calculate angles between Euler vectors\n",
        "    for i in range(len(euler_cols)):\n",
        "        for j in range(i+1, len(euler_cols)):\n",
        "            col1, col2 = euler_cols[i], euler_cols[j]\n",
        "            df[f'EulerAngle_{col1}_{col2}'] = df.apply(lambda row: angle_between(np.array([row[col1.lower()]]), np.array([row[col2.lower()]])), axis=1)\n",
        "\n",
        "    return df\n",
        "\n",
        "# Function to process all files in a directory\n",
        "def process_advanced_features(input_folder, output_folder):\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    for filename in os.listdir(input_folder):\n",
        "        if filename.endswith('.csv'):\n",
        "            df = pd.read_csv(os.path.join(input_folder, filename))\n",
        "\n",
        "            # Perform advanced feature extraction\n",
        "            df = advanced_feature_extraction(df)\n",
        "\n",
        "            # Save the DataFrame to a new CSV file in the output folder\n",
        "            new_filename = f\"advanced_{filename}\"\n",
        "            df.to_csv(os.path.join(output_folder, new_filename), index=False)\n",
        "\n",
        "# Paths to the input and output folders (Update these paths)\n",
        "input_train_path = '/content/drive/MyDrive/MSc Project/Formated_Data/Experiment_1/TrainTestDataset/Processed_Train'\n",
        "input_test_path = '/content/drive/MyDrive/MSc Project/Formated_Data/Experiment_1/TrainTestDataset/Processed_Test'\n",
        "output_train_path = '/content/drive/MyDrive/MSc Project/Formated_Data/Experiment_1/TrainTestDataset/Advanced_Features_Train'\n",
        "output_test_path = '/content/drive/MyDrive/MSc Project/Formated_Data/Experiment_1/TrainTestDataset/Advanced_Features_Test'\n",
        "\n",
        "# Run the function for train and test datasets\n",
        "process_advanced_features(input_test_path, output_test_path)\n"
      ],
      "metadata": {
        "id": "4IfEoyI_jr4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code to duplicate the Processed_Test & Processed_Train folders and modify the CSV files (deleting the PlaybackTime and Timestamp columns)\n"
      ],
      "metadata": {
        "id": "ExHPTmCDu-GE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define your paths\n",
        "base_path = '/content/drive/MyDrive/MSc Project/Formated_Data/Experiment_1/TrainTestDataset/'  # Replace with your folder path\n",
        "train_folder = os.path.join(base_path, 'Processed_Train')\n",
        "test_folder = os.path.join(base_path, 'Processed_Test')\n",
        "\n",
        "# Duplicate folders\n",
        "train_duplicate_folder = os.path.join(base_path, 'Processed_Train_Duplicate')\n",
        "test_duplicate_folder = os.path.join(base_path, 'Processed_Test_Duplicate')\n",
        "\n",
        "# Create new folders if they don't exist\n",
        "if not os.path.exists(train_duplicate_folder):\n",
        "    os.makedirs(train_duplicate_folder)\n",
        "if not os.path.exists(test_duplicate_folder):\n",
        "    os.makedirs(test_duplicate_folder)\n",
        "\n",
        "# Function to copy only CSV files from one folder to another\n",
        "def copy_csv_files(src_folder, dest_folder):\n",
        "    for filename in os.listdir(src_folder):\n",
        "        if filename.endswith('.csv'):\n",
        "            src_filepath = os.path.join(src_folder, filename)\n",
        "            dest_filepath = os.path.join(dest_folder, filename)\n",
        "            shutil.copy2(src_filepath, dest_filepath)\n",
        "\n",
        "# Copy CSV files to create duplicates\n",
        "copy_csv_files(train_folder, train_duplicate_folder)\n",
        "copy_csv_files(test_folder, test_duplicate_folder)\n",
        "\n",
        "# Function to delete specific columns from all CSV files in a folder\n",
        "def delete_columns_from_csv(folder_path):\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith('.csv'):\n",
        "            filepath = os.path.join(folder_path, filename)\n",
        "            df = pd.read_csv(filepath)\n",
        "\n",
        "            # Delete 'PlaybackTime' and 'Timestamp' columns if they exist\n",
        "            if 'PlaybackTime' in df.columns:\n",
        "                del df['PlaybackTime']\n",
        "            if 'Timestamp' in df.columns:\n",
        "                del df['Timestamp']\n",
        "\n",
        "            # Save the DataFrame back to the CSV file\n",
        "            df.to_csv(filepath, index=False)\n",
        "\n",
        "# Delete columns from all CSV files in the duplicate folders\n",
        "delete_columns_from_csv(train_duplicate_folder)\n",
        "delete_columns_from_csv(test_duplicate_folder)\n",
        "\n",
        "print(\"Process completed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4z9KxJx8GS5",
        "outputId": "45924ca4-02a6-42d7-d030-ab87f32bb598"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Process completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Drop Unnecessary Columns from Large Combined Dataset csv files (Both Train & Test datasets)"
      ],
      "metadata": {
        "id": "MK-NXa7sTEcZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Paths to your input and output CSV files\n",
        "input_file_path = '/content/drive/MyDrive/MSc Project/Formated_Data/Experiment_1/TrainTestDataset/Combined_Test.csv'\n",
        "output_file_path = '/content/drive/MyDrive/MSc Project/Formated_Data/Experiment_1/TrainTestDataset/Reduced_Combined_Test.csv'\n",
        "\n",
        "# Columns to be removed\n",
        "cols_to_drop = ['Timestamp', 'PlaybackTime']\n",
        "\n",
        "# Initialize variable to check for the first chunk\n",
        "first_chunk = True\n",
        "\n",
        "# Define chunk size (adjust based on your available memory)\n",
        "chunksize = 10 ** 6\n",
        "\n",
        "# Read the CSV file in chunks and drop specified columns\n",
        "for chunk in pd.read_csv(input_file_path, chunksize=chunksize):\n",
        "    chunk.drop(columns=cols_to_drop, inplace=True)\n",
        "\n",
        "    # Write to new CSV file\n",
        "    if first_chunk:\n",
        "        chunk.to_csv(output_file_path, index=False, mode='w')\n",
        "        first_chunk = False\n",
        "    else:\n",
        "        chunk.to_csv(output_file_path, index=False, mode='a', header=False)\n"
      ],
      "metadata": {
        "id": "KKyVpgDxTB_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing rows with empty columns, and then overwrite the original file in parts"
      ],
      "metadata": {
        "id": "DQx-llauZrVL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "file_path = '/content/drive/MyDrive/MSc Project/Formated_Data/Experiment_1/TrainTestDataset/Reduced_Combined_Test.csv'\n",
        "\n",
        "# Chunk size\n",
        "chunk_size = 10**6  # Modify this value based on available memory (Colab has 12 GB, so 10^6 size is base)\n",
        "\n",
        "# Temporary file path\n",
        "temp_file_path = '/content/drive/MyDrive/MSc Project/Formated_Data/Experiment_1/TrainTestDataset/temp_Reduced_Combined_Test.csv'\n",
        "\n",
        "# Read the large CSV file in chunks\n",
        "first_chunk = True\n",
        "for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
        "    # Remove rows with any empty columns\n",
        "    chunk.dropna(inplace=True)\n",
        "\n",
        "    # Save the cleaned chunk to a new CSV file\n",
        "    if first_chunk:\n",
        "        chunk.to_csv(temp_file_path, mode='w', index=False)\n",
        "        first_chunk = False\n",
        "    else:\n",
        "        chunk.to_csv(temp_file_path, mode='a', header=False, index=False)\n",
        "\n",
        "# Replace the original file with the cleaned file\n",
        "os.remove(file_path)\n",
        "os.rename(temp_file_path, file_path)\n",
        "\n",
        "print(f\"Completed cleaning {file_path}.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abXtRCGFZliU",
        "outputId": "96cdca07-95db-4483-9a02-3f68be1f6bd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed cleaning /content/drive/MyDrive/MSc Project/Formated_Data/Experiment_1/TrainTestDataset/Reduced_Combined_Test.csv.\n"
          ]
        }
      ]
    }
  ]
}