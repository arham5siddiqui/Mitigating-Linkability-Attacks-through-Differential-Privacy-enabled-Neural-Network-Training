{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1YUMXK2ZmRIZI0K3RG9utcgZqshGzI_Ii",
      "authorship_tag": "ABX9TyP644I2hh3E1QWA6sgmQezL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arham5siddiqui/Mitigating-Linkability-Attacks-through-Differential-Privacy-enabled-Neural-Network-Training/blob/main/Step3_User_Identification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "3VNPgEjY8N-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shuffling a Train file in chunks, in order include multiple Classes(multiple user data) in every chunk."
      ],
      "metadata": {
        "id": "6BOGaZGq5I37"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For running the model training, shuffling the users is necessary in order to get all variety of users and data in every chunk. As the file cannot be trained by taking its complete size, training one chunk at a time is possible.\n",
        "# Few errors came up initially, but after referring stack overflow blogs, error solutions and debugging, the following code was concluded.\n",
        "\n",
        "input_file_path = \"/content/drive/MyDrive/MSc Project/Formated_Data/Experiment_1/TrainTestDataset/Reduced_Combined_Train.csv\"\n",
        "output_file_path = \"/content/drive/MyDrive/MSc Project/Formated_Data/Experiment_1/TrainTestDataset/Shuffled_Reduced_Combined_Train.csv\"\n",
        "\n",
        "chunk_size = 10000  # Choose a reasonable chunk size\n",
        "\n",
        "# Initialize writing to CSV\n",
        "first_one = True\n",
        "\n",
        "# Read and shuffle each chunk, then append to new CSV\n",
        "for chunk in pd.read_csv(input_file_path, chunksize=chunk_size):\n",
        "    shuffled_chunk = chunk.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "    if first_one:\n",
        "        shuffled_chunk.to_csv(output_file_path, mode='w', index=False)\n",
        "        first_one = False\n",
        "    else:\n",
        "        shuffled_chunk.to_csv(output_file_path, mode='a', header=False, index=False)\n"
      ],
      "metadata": {
        "id": "MhMIgyeE5Jjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using: all columns except labels 'userID' and 'videoID', as features.\n",
        "First each classifier is trained,\n",
        "and then tested for each classifier."
      ],
      "metadata": {
        "id": "8wybz58msdt_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize classifiers\n",
        "classifiers = {\n",
        "    \"SGD\": SGDClassifier(),\n",
        "    \"GaussianNB\": GaussianNB(),\n",
        "    \"RandomForest\": RandomForestClassifier(),\n",
        "    \"DecisionTree\": DecisionTreeClassifier(),\n",
        "    \"KNN\": KNeighborsClassifier()\n",
        "}\n",
        "\n",
        "# Initialize a flag to indicate if each classifier has been fitted\n",
        "fitted = {name: False for name in classifiers.keys()}\n",
        "\n",
        "# Initialize empty dictionary to store accuracy\n",
        "accuracy_dict = {'Classifier': [], 'Dataset': [], 'Accuracy': []}\n",
        "\n",
        "# Initialize empty list to store train accuracy\n",
        "train_accuracy_list = []\n",
        "\n",
        "# Initialize final dictionary to store average accuracies\n",
        "final_dict = {'Classifier': [], 'Dataset': [], 'Accuracy': [], 'Train Accuracy': []}\n",
        "\n",
        "# Initialize the list of dataset types\n",
        "dataset_types = ['Quaternion', 'Euler', 'Yaw']  # Adding actual dataset types\n",
        "\n",
        "# Define chunk size\n",
        "chunk_size = 10 ** 4  # Adjust based on your available memory\n",
        "subset_size = 10 ** 4  # Size for training classifiers that don't support partial_fit\n",
        "\n",
        "# Read a small portion of the file to get the unique classes\n",
        "unique_classes = set()\n",
        "for chunk in pd.read_csv(\"/content/drive/MyDrive/MSc Project/Formated_Data/Experiment_1/TrainTestDataset/Shuffled_Reduced_Combined_Train.csv\", chunksize=chunk_size):\n",
        "    unique_classes.update(chunk['userID'])\n",
        "unique_classes = np.array(list(unique_classes))\n",
        "\n",
        "\n",
        "# Function to train classifiers that support partial_fit\n",
        "def train_partial_fit(chunk):\n",
        "    X_train = chunk.drop(['userID', 'videoID'], axis=1)\n",
        "    y_train = chunk['userID']\n",
        "    if len(np.unique(y_train)) > 1:  # Check for multiple unique classes\n",
        "        for name, clf in classifiers.items():\n",
        "            if hasattr(clf, 'partial_fit'):\n",
        "                clf.partial_fit(X_train, y_train, classes=unique_classes)\n",
        "                fitted[name] = True  # Mark as fitted\n",
        "\n",
        "# Function to test classifiers\n",
        "def test_classifiers(chunk, dataset_type):\n",
        "    X_test = chunk.drop(['userID', 'videoID'], axis=1)\n",
        "    y_test = chunk['userID']\n",
        "    for name, clf in classifiers.items():\n",
        "        if fitted[name]:  # Only test if the classifier was fitted\n",
        "            y_pred = clf.predict(X_test)\n",
        "            accuracy = accuracy_score(y_test, y_pred)\n",
        "            accuracy_dict['Classifier'].append(name)\n",
        "            accuracy_dict['Dataset'].append(dataset_type)\n",
        "            accuracy_dict['Accuracy'].append(accuracy)\n",
        "\n",
        "\n",
        "# Training loop for classifiers that support partial_fit\n",
        "for chunk in pd.read_csv(\"/content/drive/MyDrive/MSc Project/Formated_Data/Experiment_1/TrainTestDataset/Shuffled_Reduced_Combined_Train.csv\", chunksize=chunk_size):\n",
        "    train_partial_fit(chunk)\n",
        "\n",
        "# Fit classifiers that do not support partial_fit on a smaller subset\n",
        "subset = pd.read_csv(\"/content/drive/MyDrive/MSc Project/Formated_Data/Experiment_1/TrainTestDataset/Shuffled_Reduced_Combined_Train.csv\", nrows=subset_size)\n",
        "X_subset = subset.drop(['userID', 'videoID'], axis=1)\n",
        "y_subset = subset['userID']\n",
        "for name, clf in classifiers.items():\n",
        "    if not fitted[name]:\n",
        "        clf.fit(X_subset, y_subset)\n",
        "        fitted[name] = True\n",
        "        train_accuracy = clf.score(X_subset, y_subset)\n",
        "        train_accuracy_list.append((name, train_accuracy))\n",
        "\n",
        "# Test the classifiers\n",
        "for chunk in pd.read_csv(\"/content/drive/MyDrive/MSc Project/Formated_Data/Experiment_1/TrainTestDataset/Reduced_Combined_Test.csv\", chunksize=chunk_size):\n",
        "    test_classifiers(chunk, 'Test')\n",
        "\n",
        "# Average the test accuracy and fill in the dictionary for DataFrame creation\n",
        "for name in classifiers.keys():\n",
        "    indices = [i for i, x in enumerate(accuracy_dict['Classifier']) if x == name]\n",
        "    if indices:\n",
        "        avg_accuracy = np.mean([accuracy_dict['Accuracy'][i] for i in indices])\n",
        "        final_dict['Classifier'].append(name)\n",
        "        final_dict['Dataset'].append('Test')  # This will be 'Test' for all classifiers in this case\n",
        "        final_dict['Accuracy'].append(avg_accuracy)\n",
        "        final_dict['Train Accuracy'].append(np.nan)  # Placeholder for train accuracy\n",
        "\n",
        "# Incorporate the train_accuracy_list into the final_dict\n",
        "for name, train_accuracy in train_accuracy_list:\n",
        "    if name in final_dict['Classifier']:\n",
        "        idx = final_dict['Classifier'].index(name)\n",
        "        final_dict['Train Accuracy'][idx] = train_accuracy\n",
        "\n",
        "# Save results to CSV\n",
        "results_df = pd.DataFrame(final_dict)\n",
        "results_df.to_csv(\"/content/drive/MyDrive/MSc Project/Formated_Data/Experiment_1/TrainTestDataset/results.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "UuGfD-Kh_ksG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}